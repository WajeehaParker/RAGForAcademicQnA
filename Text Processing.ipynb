{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_files(folder_path):\n",
    "    tokenized_text = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                # Tokenize into sentences\n",
    "                sentences = sent_tokenize(text)\n",
    "                # Clean and tokenize each sentence\n",
    "                cleaned_sentences = []\n",
    "                for sentence in sentences:\n",
    "                    # Remove punctuation and irrelevant characters\n",
    "                    cleaned_sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "                    # Convert to lowercase and tokenize\n",
    "                    tokens = cleaned_sentence.lower().split()\n",
    "                    # Remove stopwords\n",
    "                    tokens = [token for token in tokens if token not in stop_words]\n",
    "                    cleaned_sentences.append(tokens)\n",
    "                tokenized_text.append(cleaned_sentences)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save tokens to a text file\n",
    "def save_tokens_to_file(tokenized_text, filename):\n",
    "    directory = os.path.dirname(filename)\n",
    "    try:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            for book in tokenized_text:\n",
    "                for sentence_tokens in book:\n",
    "                    for token in sentence_tokens:\n",
    "                        file.write(token + ' ')\n",
    "                    file.write('\\n')\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text for O-Level book:\n",
      "['resource', 'endorsed', 'cambridge', 'assessment', 'international', 'education', 'provides', 'support', 'option', 'b', 'cambridge', 'igcse', 'igcse', '91', 'level', 'syllabuses', '047009772147', 'examination', '2020']\n"
     ]
    }
   ],
   "source": [
    "olevel_folder = \"ConvertedBooks/Olevel\"\n",
    "olevel_tokenized_text = process_text_files(olevel_folder)\n",
    "print(\"Tokenized text for O-Level book:\")\n",
    "print(olevel_tokenized_text[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens saved to files successfully.\n"
     ]
    }
   ],
   "source": [
    "# save to text file\n",
    "\n",
    "olevel_output_file = \"olevel_tokens.txt\"\n",
    "save_tokens_to_file(olevel_tokenized_text, \"GeneratedTokens/Olevel/Book.txt\")\n",
    "print(\"Tokens saved to files successfully.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Indexing #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(tokenized_text):\n",
    "    inverted_index = defaultdict(list)\n",
    "    for i, book in enumerate(tokenized_text):\n",
    "        for j, sentence_tokens in enumerate(book):\n",
    "            for token in sentence_tokens:\n",
    "                inverted_index[token].append((i, j))\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "O-Level books containing the word 'mathematics' are indexed at:\n",
      "[(0, 7035), (0, 8418), (0, 8419)]\n"
     ]
    }
   ],
   "source": [
    "olevel_inverted_index = create_inverted_index(olevel_tokenized_text)\n",
    "word = 'mathematics'\n",
    "print(\"\\nO-Level books containing the word '{}' are indexed at:\".format(word))\n",
    "print(olevel_inverted_index.get(word, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted indices saved to files successfully.\n"
     ]
    }
   ],
   "source": [
    "# save indexing to pickle file\n",
    "\n",
    "olevel_index_file = \"Indexing/Olevel/Book.txt\"\n",
    "directory = os.path.dirname(olevel_index_file)\n",
    "try:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    with open(olevel_index_file, 'wb') as file:\n",
    "        pickle.dump(olevel_inverted_index, file)\n",
    "    print(\"Inverted indices saved to files successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
